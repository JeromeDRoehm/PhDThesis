{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31e6ac6e-30fa-49d9-9c3e-2f46ca69d117",
   "metadata": {},
   "source": [
    "# Reconstruction of Stimulus Spaces from Neural Activation Sequences\n",
    "\n",
    "This notebook contains the relevant and referenced code for Jerome Roehm's PhD Dissertation: Reconstruction of Stimulus Spaces from Neural Activation Sequences and Anti-Geometric Persistence. University of Delaware, Spring 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d675bd12-2bc3-4fa0-ab48-7137419c4212",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Imports and background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f245ff4a-5ea8-465d-a3d3-676c6c297bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import scipy as sp\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from matplotlib import collections  as mc\n",
    "import dionysus as d\n",
    "from matplotlib.text import TextPath\n",
    "import matplotlib.patches as patches\n",
    "import time\n",
    "from itertools import chain, combinations, product\n",
    "import operator\n",
    "import copy\n",
    "import random\n",
    "from scipy.linalg import eigh\n",
    "from scipy.stats import gaussian_kde\n",
    "import seaborn as sns\n",
    "from sklearn import svm\n",
    "\n",
    "#background\n",
    "\n",
    "#distinct colors for illustrations/labeling\n",
    "color_list=['#e6194B', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4', '#42d4f4', '#f032e6',\n",
    "            '#bfef45', '#fabebe', '#469990', '#e6beff', '#9A6324', '#fffac8', '#800000', '#aaffc3',\n",
    "            '#808000', '#ffd8b1', '#e60075', '#a9a9a9']\n",
    "\n",
    "#list of alphabet for labeling purposes\n",
    "alphabet=['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z',\n",
    "          'a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0365c2d-74c1-4ab7-8045-597f5e5991a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Place cell data as sequences\n",
    "\n",
    "This section contains code for analyzing and simulating place cell data in the form of discrete sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61f8e58-a0d9-426e-87c4-20a0d0f8c1f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Sequences to coactivity matrix\n",
    "\n",
    "Methods for generating a coactivity matrix from a list of sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84f86b67-5758-42f8-8afc-b4ca6d25e680",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "\n",
    "#eliminates repeated terms in the sequence example: [3,2,3,3,3,4] becomes [3,2,3,4]\n",
    "def elim_rep(data):\n",
    "    seqs = copy.deepcopy(data)\n",
    "    for seq in seqs:\n",
    "        delete_us=[]\n",
    "        for i in range(1,len(seq)):\n",
    "            if seq[i] == seq[i-1]:\n",
    "                delete_us.append(i)\n",
    "        for index in sorted(delete_us, reverse = True):\n",
    "            del seq[index]\n",
    "    return seqs\n",
    "\n",
    "#eliminates sequences of length 1\n",
    "def elim_singles(data):\n",
    "    final = []\n",
    "    for seq in data:\n",
    "        if len(seq) > 1:\n",
    "            final.append(seq)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "364721e2-096e-4bfb-b15a-22c4880bea43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main function\n",
    "\n",
    "#data given in list of sequences, output is matrix (or weighted graph) indexed by sorted elements in sequence\n",
    "#with (i,j) entry equal to number of times i appears adjacent to j in sequence set (weight of edge in graph)\n",
    "#method of adj_cor is slightly different.\n",
    "def seqs_to_cor(data, rtn = 'mtx', method = 'adj_cor', almost_adj_weight = 0.2, normalize=True): \n",
    "    seqs = elim_rep(data)\n",
    "    \n",
    "    nodes=set()\n",
    "    for seq in seqs:\n",
    "        for elmt in seq:\n",
    "            nodes.add(elmt)\n",
    "    nodes=list(nodes)\n",
    "    nodes.sort()\n",
    "    num_nodes = len(nodes)\n",
    "    \n",
    "    cor_mtx = np.zeros((num_nodes,num_nodes))\n",
    "    \n",
    "    if method == 'total_adj':\n",
    "        for seq in seqs:\n",
    "            if len(seq) >= 2:\n",
    "                for i in range(len(seq)-1):\n",
    "                    cor_mtx[nodes.index(seq[i]),nodes.index(seq[i+1])] += 1\n",
    "                    cor_mtx[nodes.index(seq[i+1]),nodes.index(seq[i])] += 1\n",
    "    elif method == 'adj_cor':\n",
    "        for i in range(len(nodes)):\n",
    "            for j in range(i+1,len(nodes)):\n",
    "                total_app = 0\n",
    "                total_tog = 0\n",
    "                for seq in seqs:\n",
    "                    if len(seq) >= 2:\n",
    "                        for k in range(len(seq)):\n",
    "                            if seq[k] in set([nodes[i], nodes[j]]):\n",
    "                                if seq[k] == nodes[i]:\n",
    "                                    other = nodes[j]\n",
    "                                else:\n",
    "                                    other = nodes[i]\n",
    "                                total_app += 1\n",
    "                                if k > 0 and seq[k-1] == other:\n",
    "                                    total_tog += 1\n",
    "                                elif k > 1 and seq[k-2] == other:\n",
    "                                    total_tog += almost_adj_weight\n",
    "                                elif k < len(seq)-1 and seq[k+1] == other:\n",
    "                                    total_tog += 1\n",
    "                                elif k < len(seq) - 2 and seq[k+2] == other:\n",
    "                                    total_tog += almost_adj_weight\n",
    "                if total_app>0:\n",
    "                    cor_mtx[i,j] = total_tog / total_app\n",
    "        cor_mtx = cor_mtx + np.transpose(cor_mtx)\n",
    "    \n",
    "    for i in range(num_nodes):\n",
    "        cor_mtx[i,i] = 0\n",
    "        \n",
    "    if normalize:\n",
    "        m=np.max(cor_mtx)\n",
    "        if m != 0:\n",
    "            cor_mtx=cor_mtx/m\n",
    "        \n",
    "    if rtn == 'mtx':\n",
    "        return cor_mtx\n",
    "    \n",
    "    elif rtn == 'nodes_mtx':\n",
    "        return nodes, cor_mtx\n",
    "    \n",
    "    elif rtn == 'graph':\n",
    "        G=nx.empty_graph()\n",
    "        for n in nodes:\n",
    "            G.add_node(n)\n",
    "        for i in range(num_nodes):\n",
    "            for j in range(i):\n",
    "                if cor_mtx[i,j] != 0:\n",
    "                    G.add_edge(nodes[i],nodes[j], weight = cor_mtx[i,j])\n",
    "        return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a822c5-dbe4-4e09-8288-f4a249127944",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Discrete methods for analyzing the constructability of a coactivity matrix or graph\n",
    "\n",
    "Discrete methods to evaluating how constructible a matrix or graph is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8367f48-77fa-4409-bb75-3e8b353f9f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "# from characterization of constructible graphs (c)(i) in the dissertation\n",
    "def fixed_kiss_test_1(data,prt=False):\n",
    "    G=data_to_graph(data)\n",
    "    #testing single nodes\n",
    "    for node in G.nodes():\n",
    "        neighbors=set(G.neighbors(node))\n",
    "        if len(neighbors)>=3:\n",
    "            for s in combinations(neighbors,3):\n",
    "                flag=0\n",
    "                for p in s:\n",
    "                    for q in s:\n",
    "                        if flag==0:\n",
    "                            if G.has_edge(p,q):\n",
    "                                flag=1\n",
    "                if flag==0:\n",
    "                    if prt:\n",
    "                        print('Node',node,'is connected to nodes',list(s),'none of which are connected to each other')\n",
    "                    return False\n",
    "    \n",
    "    #testing cliques\n",
    "    for clq in nx.find_cliques(G):\n",
    "        clq_neighbors=set([])\n",
    "        for n in clq:\n",
    "            clq_neighbors=clq_neighbors.union(set(list(G.neighbors(n))))\n",
    "        for n in clq:\n",
    "            if n in clq_neighbors:\n",
    "                clq_neighbors.remove(n)\n",
    "        if len(clq_neighbors)>=3:\n",
    "            for s in combinations(clq_neighbors,3):\n",
    "                flag=0\n",
    "                for p in s:\n",
    "                    for q in s:\n",
    "                        if flag==0:\n",
    "                            if G.has_edge(p,q):\n",
    "                                flag=1\n",
    "                if flag==0:\n",
    "                    if prt:\n",
    "                        print('Clique',list(clq),'is connected to nodes',list(s),'none of which are connected to each other')\n",
    "                    return False\n",
    "    return True\n",
    "\n",
    "# reduces data in the form of sequences, adjacency matrix, or *graph* (mainly for node naming)\n",
    "def data_to_graph(data):\n",
    "    if type(data)==list:\n",
    "        adj_mtx,node_labels=reduce_seqs_smart(data)\n",
    "        G=nx.from_numpy_matrix(adj_mtx)\n",
    "    elif type(data)==np.ndarray:\n",
    "        adj_mtx=data.copy()\n",
    "        node_labels=list(range(adj_mtx.shape[0]))\n",
    "        G=nx.from_numpy_matrix(adj_mtx)\n",
    "    elif type(data)==type(nx.complete_graph(1)):\n",
    "        return data\n",
    "    labels={}\n",
    "    for i in range(len(node_labels)):\n",
    "        labels[i]=node_labels[i]\n",
    "    G=nx.relabel_nodes(G,labels)\n",
    "    return G\n",
    "\n",
    "#(mainly for node naming)\n",
    "def reduce_seqs_smart(seqs):\n",
    "    nodes_set=set()\n",
    "    for seq in seqs:\n",
    "        for n in seq:\n",
    "            nodes_set.add(n)\n",
    "    nodes=list(nodes_set)\n",
    "    nodes.sort()\n",
    "    num_nodes=len(nodes)\n",
    "    adj_mtx=np.zeros((num_nodes,num_nodes))\n",
    "    for seq in seqs:\n",
    "        for i in range(len(seq)-1):\n",
    "            adj_mtx[nodes.index(seq[i]),nodes.index(seq[i+1])]=1\n",
    "    adj_mtx=adj_mtx+adj_mtx.T\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_nodes):\n",
    "            if i==j:\n",
    "                adj_mtx[i,j]=0\n",
    "            elif adj_mtx[i,j] != 0:\n",
    "                adj_mtx[i,j]=1\n",
    "    return adj_mtx, nodes\n",
    "\n",
    "#helps with finding lowest cost alteration, this is a well known NP hard problem\n",
    "def subset_sum(numbers, pairing, target, partial=[], code_partial=[], sols=False):\n",
    "    s = sum(partial)\n",
    "    \n",
    "    if sols == False:\n",
    "        sols = []\n",
    "\n",
    "    # check if the partial sum is equals to target\n",
    "    if s == target: \n",
    "        sols.append(code_partial)\n",
    "        \n",
    "    if s >= target:\n",
    "        return  # if we reach the number, stop\n",
    "\n",
    "    for i in range(len(numbers)):\n",
    "        n = numbers[i]\n",
    "        code = pairing[i]\n",
    "        subset_sum(numbers[i+1:], pairing[i+1:], target, partial + [n],code_partial + [code], sols=sols)\n",
    "\n",
    "    return sols\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf1d0548-fec2-4ffc-a05c-16aabebce802",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main functions\n",
    "\n",
    "#test for passing chordal [constructability characterization (c)(ii)] and kiss_test [(c)(i)]\n",
    "def is_constr(data):\n",
    "    G_0=data_to_graph(data)\n",
    "    if fixed_kiss_test_1(G_0) == False:\n",
    "        return False\n",
    "    if nx.is_chordal(G_0) == False:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Assessing how close data is to constructible using combinatorial methods to \"fix\" the graph.\n",
    "def fix_score(data, percentile=10, thresh_with_mean = False, rtn='both', normalize = True,\n",
    "              fast=True, method = 'adj_cor', almost_adj_weight = 0.2, constr_test='theory',\n",
    "              time_limit = 10**5, fast_binary=False):\n",
    "    start=time.time()\n",
    "    if type(data) == list:\n",
    "        G=seqs_to_cor(data, rtn='graph', method = method, almost_adj_weight = almost_adj_weight)\n",
    "    elif type(data) == np.ndarray:\n",
    "        G=nx.from_numpy_matrix(data)\n",
    "    elif type(data) == type(nx.complete_graph(1)):\n",
    "        G=data.copy()\n",
    "        \n",
    "    num_nodes = G.number_of_nodes()\n",
    "    num_pos_edges = num_nodes * (num_nodes-1) / 2\n",
    "    \n",
    "    # if graph is unweighted, assign unit weights\n",
    "    if nx.is_weighted(G) == False:\n",
    "        for edge in G.edges():\n",
    "            G[edge[0]][edge[1]]['weight']=1.0\n",
    "    \n",
    "    node_list = list(G.nodes())\n",
    "    edge_dict = nx.get_edge_attributes(G,'weight')\n",
    "    edge_list = list(edge_dict.keys())\n",
    "    weight_list = list(edge_dict.values())\n",
    "    \n",
    "    if fast:\n",
    "        #going to fudge a bit here for discrete-ness and speed\n",
    "        # by making edge weights integers on a scale of 0 - 23\n",
    "        if len(weight_list) > 0:\n",
    "            weight_list = np.rint(weight_list / np.max(weight_list) * 23)\n",
    "        for e in range(len(edge_list)):\n",
    "            if type(edge_list[e][0]) == np.ndarray:\n",
    "                edge_list[e][0], edge_list[e][1] = tuple(edge_list[e][0]), tuple(edge_list[e][1])\n",
    "            edge_dict[(edge_list[e][0],edge_list[e][1])] = weight_list[e]\n",
    "    \n",
    "    weights_with_non_edges = list(weight_list)\n",
    "    \n",
    "    while len(weights_with_non_edges) < num_pos_edges:\n",
    "        weights_with_non_edges.append(0)\n",
    "    \n",
    "    thresh_val = round(np.percentile(np.array(weights_with_non_edges),percentile))\n",
    "    \n",
    "    if thresh_with_mean:\n",
    "        thresh_val = (thresh_val + mean(weight_list))/2\n",
    "\n",
    "    thresh_weights = weight_list - thresh_val\n",
    "    edge_alteration_costs = abs(thresh_weights)\n",
    "    \n",
    "    for i in range(len(edge_alteration_costs)):\n",
    "        if edge_alteration_costs[i] == 0:\n",
    "            edge_alteration_costs[i] = 1\n",
    "    \n",
    "    TG = nx.empty_graph(0)\n",
    "    for n in list(G.nodes()):\n",
    "        TG.add_node(n)\n",
    "    for i in range(len(edge_list)):\n",
    "        if thresh_weights[i] > 0:\n",
    "            TG.add_edge(edge_list[i][0],edge_list[i][1],weight=thresh_weights[i])\n",
    "    \n",
    "    if is_constr(TG):\n",
    "        if rtn == 'both':\n",
    "            return 0,[TG]\n",
    "    \n",
    "    best_fixes = []\n",
    "    found = False\n",
    "    cost = 1\n",
    "    sum_edge_alt = sum(edge_alteration_costs)\n",
    "    binary_max = .05 * sum_edge_alt\n",
    "    \n",
    "    while found == False and (time.time()-start)/60 < time_limit:\n",
    "        poss_alterations = subset_sum(edge_alteration_costs,edge_list,cost)\n",
    "        for alter_us in poss_alterations:\n",
    "            TG1 = TG.copy()\n",
    "            for edge in alter_us:\n",
    "                if TG1.has_edge(edge[0],edge[1]):\n",
    "                    TG1.remove_edge(edge[0],edge[1])\n",
    "                else:\n",
    "                    TG1.add_edge(edge[0],edge[1],weight=edge_dict[(edge[0],edge[1])])\n",
    "            if constr_test == 'theory':\n",
    "                if is_constr(TG1):\n",
    "                    found = True\n",
    "                    best_fixes.append(TG1)\n",
    "                    if rtn == 'cost' or fast == True:\n",
    "                        break\n",
    "            elif constr_test == 'iter':\n",
    "                if iter_construct_in_R1(TG1, use_as_test=True):\n",
    "                    found = True\n",
    "                    best_fixes.append(TG1)\n",
    "                    if rtn == 'cost' or fast == True:\n",
    "                        break\n",
    "        if found == False:\n",
    "            cost += 1\n",
    "        if fast_binary and cost > binary_max:\n",
    "            break\n",
    "    \n",
    "    if len(best_fixes)==0:\n",
    "        #add small weight edges to complete the graph. minimally messes up cont optimize\n",
    "        for i in range(len(node_list)):\n",
    "            for j in range(i+1,len(node_list)):\n",
    "                if G.has_edge(node_list[i],node_list[j]) == False:\n",
    "                    G.add_edge(node_list[i],node_list[j], weight = 0.0001)\n",
    "        best_fixes.append(G)\n",
    "        cost = sum(edge_alteration_costs) * 0.5 #this is max cost for complete bipartite\n",
    "                                                # serves as flag that the time limit was exceeded\n",
    "    \n",
    "    if normalize:\n",
    "        cost = cost / sum(edge_alteration_costs)\n",
    "    \n",
    "    if rtn == 'cost':\n",
    "        return cost\n",
    "    if rtn == 'both':\n",
    "        return cost , best_fixes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fa395f-d62d-42b4-92db-ebf0a85750ab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Simulating place cell sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d7515dd-dc3c-4493-8e9d-0007f2cd4c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "#bump function given current position of animal and list of positions of centers of unit intervals\n",
    "def bump(c_pos,i_pos):\n",
    "    array = np.array(i_pos)\n",
    "    d=np.clip(c_pos - i_pos,-0.4999999999,0.4999999999)\n",
    "    return np.exp((4*np.square(d) / (4*np.square(d)-1)))\n",
    "\n",
    "# helper function to compute direction\n",
    "def get_dir(a,b):\n",
    "    if abs((a-b)[1]) < 10**-6:\n",
    "        if (a-b)[0] < 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return math.pi\n",
    "    if abs((a-b)[0]) < 10**-6:\n",
    "        if (a-b)[1] < 0:\n",
    "            return math.pi/2\n",
    "        else:\n",
    "            return 3*math.pi/2\n",
    "    theta = np.arctan((b[1]-a[1])/(b[0]-a[0]))\n",
    "    if a[0] - b[0] > 0:\n",
    "        theta = (theta + math.pi) % (2*math.pi)\n",
    "    return theta\n",
    "\n",
    "#2d bump function\n",
    "def bump_2d(c_pos,disc_center):\n",
    "    d = np.linalg.norm(c_pos-disc_center)\n",
    "    if d >= 0.5:\n",
    "        return 0\n",
    "    return np.exp((4*np.square(d) / (4*np.square(d)-1)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ac9ebf3-e103-48c5-a9c4-250c58795b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main functions\n",
    "\n",
    "# generate sequences in a strictly one-dimensional environment\n",
    "#generates some sequences in a mildly realistic way, and shows how they were generated\n",
    "def generate_seqs(pos_list,labels,start_pos='random',travel_length='random',\n",
    "                  num_thoughts=10,num_seqs=10, show = False, save=False, figsize=(15,3), title='Title'):\n",
    "    track_length = max(pos_list) + 0.5\n",
    "    seqs = []\n",
    "    paths = []\n",
    "    for c in range(num_seqs):\n",
    "        seq = []\n",
    "        \n",
    "        if start_pos != 'random':\n",
    "            x_0 = start_pos * track_length\n",
    "        else:\n",
    "            found = False\n",
    "            while found == False:\n",
    "                t = np.random.normal(.5,.35)\n",
    "                if 0<t and t<1:\n",
    "                    found = True\n",
    "                    x_0 = t * track_length\n",
    "        if travel_length == 'random':\n",
    "            length = np.random.normal(1,.2)\n",
    "            if np.random.uniform(0,track_length) < x_0:\n",
    "                length = -length\n",
    "            length = length\n",
    "        else:\n",
    "            length = travel_length * track_length\n",
    "        if num_thoughts=='random':\n",
    "            num_stops = np.random.randint(5,10)\n",
    "        else:\n",
    "            num_stops=num_thoughts\n",
    "        checkpts = np.linspace(x_0,np.clip(x_0+length,0,track_length),num_stops) #length contains direction + / -\n",
    "        for x in checkpts:\n",
    "            signal = []\n",
    "            for i in range(len(pos_list)):\n",
    "                if bump(x,pos_list[i]) > np.random.uniform(0,1):\n",
    "                    signal.append(labels[i])\n",
    "            random.shuffle(signal)\n",
    "            for l in signal:\n",
    "                seq.append(l)\n",
    "        seqs.append(seq)\n",
    "        paths.append(checkpts)\n",
    "    \n",
    "    if show:\n",
    "        vals=np.zeros((1000,len(labels)))\n",
    "        for j in range(1000):\n",
    "            vals[j,:]=bump(np.linspace(min(min(pos_list)-0.5,0),track_length,1000)[j],np.array(pos_list))\n",
    "        plt.figure(figsize=figsize, dpi=200)\n",
    "        #plt.title(title,size=15)\n",
    "        for i in range(len(labels)):\n",
    "            if type(labels[i]) in set([int,float]):\n",
    "                color_index = labels[i]%20\n",
    "            else:\n",
    "                color_index = i%20\n",
    "            plt.plot(np.linspace(min(min(pos_list)-0.5,0),track_length,1000),vals[:,i], color = color_list[color_index%20])\n",
    "            plt.plot(pos_list[i],1,marker=TextPath((-4,-2), str(labels[i])),markersize=20*np.sqrt(len(str(labels[i]))),color=color_list[color_index%20],alpha=0.5)\n",
    "            plt.plot([0],[1.1],color='#FFFFFF')\n",
    "        for i in range(len(paths)):\n",
    "            plt.plot(paths[i],len(paths[i])*[i/10 + .05], marker = 'o', color = 'k', alpha = 0.3)\n",
    "        #plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        if save != False:\n",
    "            plt.savefig(save)\n",
    "        plt.show()\n",
    "    if num_seqs > 0:\n",
    "        return seqs\n",
    "    \n",
    "# function to generate and show 2d sequences\n",
    "def generate_seqs_2d(pos_array, labels, start_pos='random',travel_length='random',\n",
    "                      num_thoughts=10,num_seqs=10, momentum=1, show = False, save=False, figsize=(4,4)):\n",
    "    track_length=np.max(pos_array[0,:]) + 0.25\n",
    "    track_width=np.max(pos_array[1,:]) + 0.25\n",
    "    center = .5*np.array([track_length,track_width])\n",
    "    seqs=[]\n",
    "    paths=[]\n",
    "    for c in range(num_seqs):\n",
    "        if start_pos != 'random':\n",
    "            x_0 = start_pos\n",
    "        else:\n",
    "            x_0 = np.array([np.random.uniform(0,track_length),np.random.uniform(0,track_width)])\n",
    "        if travel_length == 'random':\n",
    "            length = np.random.normal(.7,.2) * (track_length*track_width)**0.5\n",
    "        else:\n",
    "            length = travel_length * (track_length*track_width)**0.5\n",
    "        step_length = length / num_thoughts\n",
    "        checkpts = np.zeros((2,num_thoughts))\n",
    "        cur_pos = x_0\n",
    "        checkpts[:,0] = x_0\n",
    "        cur_dir = get_dir(cur_pos,center)\n",
    "        for k in range(1,num_thoughts):\n",
    "            found_new = False\n",
    "            tries = 0\n",
    "            while found_new == False:\n",
    "                if k == 1:\n",
    "                    new_dir = cur_dir + np.random.normal(0,1.4)\n",
    "                else:\n",
    "                    new_dir = cur_dir + np.random.normal(0,1.4/(momentum+10**-5))\n",
    "                new_pos = cur_pos + step_length * np.array([math.cos(new_dir),math.sin(new_dir)])\n",
    "                if new_pos[0]>0 and new_pos[0]<track_length and new_pos[1]>0 and new_pos[1]<track_width:\n",
    "                    found_new = True\n",
    "                    checkpts[:,k] = new_pos\n",
    "                    cur_pos , cur_dir = new_pos , new_dir\n",
    "                if tries > 10**2:\n",
    "                    found_rot = False\n",
    "                    rot = 0\n",
    "                    while found_rot == False:\n",
    "                        new_dir = cur_dir + (-1)**rot * .03 * rot\n",
    "                        new_pos = cur_pos + step_length * np.array([math.cos(new_dir),math.sin(new_dir)])\n",
    "                        if new_pos[0]>0 and new_pos[0]<track_length and new_pos[1]>0 and new_pos[1]<track_width:\n",
    "                            found_new = True\n",
    "                            found_rot = True\n",
    "                            checkpts[:,k] = new_pos\n",
    "                            cur_pos , cur_dir = new_pos , new_dir\n",
    "                        rot += 1\n",
    "                tries +=1\n",
    "        paths.append(checkpts)\n",
    "        \n",
    "        seq = []\n",
    "        for j in range(len(checkpts[0,:])):\n",
    "            signal=[]\n",
    "            for i in range(len(pos_array[0,:])):\n",
    "                if bump_2d(checkpts[:,j],pos_array[:,i]) > np.random.uniform(0,1):\n",
    "                    signal.append(labels[i])\n",
    "            random.shuffle(signal)\n",
    "            for l in signal:\n",
    "                seq.append(l)\n",
    "        seqs.append(seq)\n",
    "    \n",
    "    if show:\n",
    "        plt.figure(figsize=figsize, dpi=150)\n",
    "        plt.plot([0,track_length,track_length,0,0],[0,0,track_width,track_width,0],'k',alpha = 0.5)\n",
    "        for i in range(len(paths)):\n",
    "            plt.plot(paths[i][0,0],paths[i][1,0],marker = TextPath((0,0), alphabet[i]), markersize=15*np.sqrt(len(str(i))), color='k',alpha=0.3)\n",
    "        for i in range(len(paths)):\n",
    "            plt.plot(paths[i][0,1:],paths[i][1,1:],color='k',alpha=0.3,marker='o',markersize=2)\n",
    "        for i in range(len(paths)):\n",
    "            plt.plot(paths[i][0,:2],paths[i][1,:2],color='k',alpha=0.3)\n",
    "        plt.axis('equal')\n",
    "        \n",
    "        for i in range(pos_array.shape[1]):\n",
    "            plt.plot(pos_array[0,i],pos_array[1,i],marker=TextPath((0,0), str(labels[i])),linewidth=1, markeredgewidth=.25,markeredgecolor='k', color=color_list[i%20], markersize=25*np.sqrt(len(str(labels[i]))))\n",
    "            for j in range(1,20):\n",
    "                circle = plt.Circle((pos_array[0,i], pos_array[1,i]), 0.5/20*j, color=color_list[i%20], fill=True, alpha=.22/j+.01)\n",
    "                \n",
    "                ax=plt.gca()\n",
    "                ax.add_patch(circle)\n",
    "            circle_border = plt.Circle((pos_array[0,i], pos_array[1,i]), 0.5, color=color_list[i%20], fill=False, alpha=0.15)\n",
    "            ax=plt.gca()\n",
    "            #ax.add_patch(circle_border)\n",
    "        plt.axis('off')\n",
    "        if save != False:\n",
    "            plt.savefig(save)\n",
    "        plt.show()\n",
    "        if num_seqs > 0:\n",
    "            for i in range(num_seqs):\n",
    "                print(alphabet[i%52],seqs[i])\n",
    "    if num_seqs > 0:\n",
    "        return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50c5daa-f960-44d7-a16f-b163657d0cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method and parameter choices for generating the data for the dissertation\n",
    "# will take time to reexecute\n",
    "num_nodes = 8\n",
    "num_seqs = 12\n",
    "\n",
    "sessions=1000\n",
    "\n",
    "lin_data=[]\n",
    "for ses in range(sessions):\n",
    "    i_pos = np.zeros(num_nodes)\n",
    "    for i in range(num_nodes):\n",
    "        found = False\n",
    "        while found == False:\n",
    "            new = np.random.uniform(0,num_nodes/3+0)\n",
    "            if np.min(abs(i_pos - new)) > .08:\n",
    "                found = True\n",
    "                i_pos[i]=new\n",
    "    labels = list(range(num_nodes))\n",
    "    gen_data = generate_seqs(i_pos,labels,show = False, num_seqs = num_seqs, num_thoughts = 'random',\n",
    "                            start_pos = 'random', travel_length='random', save=False)\n",
    "    lin_data.append(gen_data)\n",
    "    \n",
    "lin_scores=[]\n",
    "for gen_data in lin_data:\n",
    "    lin_scores.append(fix_score(gen_data)[0])\n",
    "    \n",
    "box_data=[]\n",
    "for ses in range(sessions):\n",
    "    rand_mesh=np.zeros((2,num_nodes))\n",
    "    for i in range(num_nodes):\n",
    "        filled=False\n",
    "        while filled==False:\n",
    "            rand=np.random.uniform(0.2,1.3,2)\n",
    "            found = True\n",
    "            for j in range(i):\n",
    "                if np.linalg.norm(rand - rand_mesh[:,j]) < .2:\n",
    "                    found = False\n",
    "            if found:\n",
    "                rand_mesh[:,i]=rand\n",
    "                filled=True\n",
    "    gen_data = generate_seqs_2d(rand_mesh,list(range(num_nodes)), show=False,\n",
    "                                travel_length='random',momentum=1.5,num_seqs=num_seqs)\n",
    "    box_data.append(gen_data)\n",
    "    \n",
    "box_scores=[]\n",
    "for gen_data in box_data:\n",
    "    box_scores.append(fix_score(gen_data)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92d3bdc-dc7c-4d15-9c1b-e9c8bd9d455b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Place cell data as spike trains\n",
    "\n",
    "This section contains code for analyzing and simulating place cell data in the form of spike trains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816f9897-b646-40c2-9fe4-be8fe749879d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Spike trains to coactivity\n",
    "\n",
    "Methods for generating a coactivity matrix from a collection of spike times for place cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d3300be-ace4-496e-8561-213d48764356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "#smoothing function for vectors, also known as the 'boxcar mean'\n",
    "def moving_mean(array,w):\n",
    "    if w == 0:\n",
    "        return array\n",
    "    new=np.zeros(len(array))\n",
    "    for i in range(len(array)):\n",
    "        new[i]=np.mean(array[max(i-w,0):min(i+w,len(array))+1])\n",
    "    return new\n",
    "\n",
    "#scaled sliding dot product\n",
    "def sliding_dot(v1,v2,max_slide,method='sum', scaled=True):\n",
    "    sliding_values=[]\n",
    "    for s in range(-max_slide,0):\n",
    "        if scaled:\n",
    "            sliding_values.append(np.dot(v1[-s:],v2[:s]) * (s/max_slide+1))\n",
    "        else:\n",
    "            sliding_values.append(np.dot(v1[-s:],v2[:s]))\n",
    "    for s in [0]:\n",
    "        sliding_values.append(np.dot(v1,v2))\n",
    "    for s in range(1,max_slide+1):\n",
    "        if scaled:\n",
    "            sliding_values.append(np.dot(v1[:-s],v2[s:]) * (-s/max_slide+1))\n",
    "        else:\n",
    "            sliding_values.append(np.dot(v1[:-s],v2[s:]))\n",
    "    if type(method) == int:\n",
    "        return np.max(moving_mean(sliding_values,method))\n",
    "    elif method == 'sum':\n",
    "        return np.sum(sliding_values)\n",
    "    elif method == 'max':\n",
    "        return np.max(sliding_values)\n",
    "    \n",
    "#pick out dead spots in activity vector\n",
    "def zero_runs(a):\n",
    "    iszero = np.concatenate(([0], np.equal(a, 0).view(np.int8), [0]))\n",
    "    absdiff = np.abs(np.diff(iszero))\n",
    "    ranges = np.where(absdiff == 1)[0].reshape(-1, 2)\n",
    "    return ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8bc3864-bc99-4bff-9ba8-83c61de47d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function\n",
    "\n",
    "# Takes data as described below\n",
    "# spike_times : a list of 1D arrays with the spike times for the neurons of interst, during times of interest\n",
    "\n",
    "#Parameters:\n",
    "# w : time window/bin size\n",
    "# abs_threshold : number of spikes in bin must exceed this, else returns to zero\n",
    "# threshold : normalized number of spikes in bin must exceed this, else returns to zero\n",
    "# mm_smooth : moving smoothing parameter for spikes in bins\n",
    "# normalize : if true, normalize the activity vectors\n",
    "# magnify_spikes : raise activity vectors to this power to magnify the spikes\n",
    "# max_rel_time : maximum time expected between adjacent or near adjacent place fields WILL NEED TO BE ADJUSTED FOR RIPPLES\n",
    "# scaled_sd : whether or not to scale the sliding dot product in proportion to the slide\n",
    "# min_inactive : minimum time to classify as inactive to partition into trials / chunks\n",
    "# method : method for totaling sliding dot product, sum or max or number for max of smoothed\n",
    "# show : show insight into the process\n",
    "# names : names for the neurons for the showing portion\n",
    "\n",
    "def discrete_correlation(spike_times, w=0.1, abs_thresh=.7, threshold=0.2,\n",
    "                         mm_smooth=3, normalize=True, magnify_spikes=1,\n",
    "                         max_rel_time=3, scaled_sd=True, save=False, title=False,\n",
    "                         min_inactive=5, method='sum', show=False, names=False,\n",
    "                         figsize=(10,4)):\n",
    "    first_spike,last_spike=10**6,0\n",
    "    for n in spike_times:\n",
    "        if len(n)>0:\n",
    "            if np.min(n)<first_spike:\n",
    "                first_spike=np.min(n)\n",
    "            if np.max(n)>last_spike:\n",
    "                last_spike=np.max(n)\n",
    "    coactivity_windows=np.arange(first_spike-w,last_spike+w,w)\n",
    "    activity_vectors=np.zeros((len(coactivity_windows)+1,len(spike_times)))\n",
    "    \n",
    "    for n in range(len(spike_times)):\n",
    "        for spk in spike_times[n]:\n",
    "            #if we wanted to consider velocity here, we could\n",
    "            activity_vectors[np.digitize(spk,coactivity_windows),n]+=1\n",
    "    \n",
    "    #smoothing to avoid \"alternating near misses\"\n",
    "    if mm_smooth != False:\n",
    "        for i in range(len(spike_times)):\n",
    "            activity_vectors[:,i]=moving_mean(activity_vectors[:,i],mm_smooth)\n",
    "            \n",
    "    # absolute threshold to remove one-off spikes\n",
    "    if abs_thresh != False:\n",
    "        for i in range(len(spike_times)):\n",
    "            activity_vectors[:,i]=activity_vectors[:,i]*(activity_vectors[:,i]>abs_thresh)\n",
    "            \n",
    "    # normalizing to account for some neurons being more active than others\n",
    "    # could get fancier later (with percentiles and such)\n",
    "    if normalize:\n",
    "        for i in range(len(spike_times)):\n",
    "            if np.max(activity_vectors[:,i])>0:\n",
    "                activity_vectors[:,i]=activity_vectors[:,i] / np.max(activity_vectors[:,i])\n",
    "    \n",
    "    #magnify the spikes and minimize \"small activity\"\n",
    "    activity_vectors=activity_vectors**magnify_spikes\n",
    "                \n",
    "    # hard threshold to account for noise\n",
    "    if threshold != False:\n",
    "        for i in range(len(spike_times)):\n",
    "            activity_vectors[:,i]=activity_vectors[:,i] * (activity_vectors[:,i]> threshold*np.max(activity_vectors[:,i]))\n",
    "\n",
    "    # transposing so that it matches the matshow images intuition\n",
    "    activity_vectors=activity_vectors.T\n",
    "    \n",
    "    #breaking into chunks so the shift does not have to be uniform for every trial, also for speed\n",
    "    cols_to_delete=[(0,0)]\n",
    "    zero_col_intervals=zero_runs(np.sum(activity_vectors,axis=0)>=.01)\n",
    "    for i in range(zero_col_intervals.shape[0]):\n",
    "        if zero_col_intervals[i,1] - zero_col_intervals[i,0] > round(1/w * min_inactive):\n",
    "            cols_to_delete.append(zero_col_intervals[i,:])\n",
    "    cols_to_delete.append((activity_vectors.shape[1]-1,activity_vectors.shape[1]-1))\n",
    "        \n",
    "    av_chunks=[]\n",
    "    for i in range(len(cols_to_delete)-1):\n",
    "        av_chunks.append(activity_vectors[:,cols_to_delete[i][1]:cols_to_delete[i+1][0]])\n",
    "    \n",
    "    #computing the extra fancy correlation matrix\n",
    "    co_mtx=np.zeros((len(spike_times),len(spike_times)))\n",
    "    max_slide = round(max_rel_time/w)\n",
    "    for av in av_chunks:\n",
    "        for i in range(len(spike_times)):\n",
    "            for j in range(i+1,len(spike_times)):\n",
    "                co_mtx[i,j] += sliding_dot(av[i,:],av[j,:],max_slide,method=method,scaled=scaled_sd)\n",
    "                \n",
    "    co_mtx=co_mtx+co_mtx.T\n",
    "    \n",
    "    if show:\n",
    "        if names==False:\n",
    "            names=list(range(len(spike_times)))\n",
    "        for i in range(len(av_chunks)):\n",
    "            fig = plt.figure(figsize=figsize, dpi=200)\n",
    "            ax = fig.add_subplot(111)\n",
    "            cax = ax.matshow(av_chunks[i],aspect=10)\n",
    "            fig.colorbar(cax)\n",
    "            labels=[]\n",
    "            for n in names:\n",
    "                labels.append(str(n))\n",
    "            ax.set_yticks(np.arange(len(labels)))\n",
    "            ax.set_yticklabels(labels)\n",
    "            ax.set_ylabel('Neurons')\n",
    "            ax.set_xlabel('Time ('+str(w)+' second bins)')\n",
    "            if title != False:\n",
    "                plt.title(title)\n",
    "            else:\n",
    "                plt.title('Chuck '+str(i))\n",
    "            if save !=False:\n",
    "                plt.savefig(save,dpi=200)\n",
    "            plt.show()\n",
    "        \n",
    "        fig = plt.figure(figsize=(4,4))\n",
    "        ax = fig.add_subplot(111)\n",
    "        cax = ax.matshow(co_mtx)\n",
    "        fig.colorbar(cax)\n",
    "        plt.show()\n",
    "    \n",
    "    return co_mtx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19555e95-0d39-4e5a-92cb-68473ef29daf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Continuous methods for analyzing the constructability of a coactivity matrix or graph\n",
    "Optimization and eigen methods to evaluating how constructible a matrix or graph is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4af553f2-78e1-480f-abd5-c5f1a35509c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def bp(x,y):\n",
    "    d=abs(x-y)\n",
    "    if d>=0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.exp(4*d**2 / (4*d**2 - 1))\n",
    "    \n",
    "def objective_function(pos_vect,cor_mtx):\n",
    "    total = 0\n",
    "    for i in range(len(pos_vect)):\n",
    "        for j in range(i+1,len(pos_vect)):\n",
    "            total += (cor_mtx[i,j] - bp((pos_vect[i]+pos_vect[j])/2,pos_vect[i]))**2\n",
    "    return total\n",
    "\n",
    "def partial_bp_xr(pos_vect,cor_mtx,j,r):\n",
    "    if abs(pos_vect[r]-pos_vect[j]) >= 1:\n",
    "        return 0\n",
    "    d = pos_vect[j] - pos_vect[r]\n",
    "    A = np.exp(d**2 / (d**2 - 1))\n",
    "    B = 2*d / (d**2 - 1)**2\n",
    "    return A*B\n",
    "\n",
    "def partial_F_xr(pos_vect,cor_mtx,r):\n",
    "    total = 0\n",
    "    for i in range(len(pos_vect)):\n",
    "        if i != r:\n",
    "            A = partial_bp_xr(pos_vect,cor_mtx,i,r)\n",
    "            B = -1*cor_mtx[r,i] + bp((pos_vect[r]+pos_vect[i])/2 , pos_vect[r])\n",
    "            total = total + 2*A*B\n",
    "    return total\n",
    "\n",
    "def partial_F(pos_vect,cor_mtx):\n",
    "    out_vect = np.zeros(len(pos_vect))\n",
    "    for r in range(len(pos_vect)):\n",
    "        out_vect[r] = partial_F_xr(pos_vect,cor_mtx,r=r)\n",
    "    return out_vect\n",
    "\n",
    "def hess_F_xr_xt(pos_vect,cor_mtx,r,t):\n",
    "    if abs(pos_vect[t] - pos_vect[r]) >= 1:\n",
    "        return 0\n",
    "    A = -1 * (partial_bp_xr(pos_vect,cor_mtx,t,r)**2)\n",
    "    B = -cor_mtx[r,t] + bp((pos_vect[t]+pos_vect[r])/2,pos_vect[r])\n",
    "    C = bp((pos_vect[t]+pos_vect[r])/2,pos_vect[r])\n",
    "    D = (-2*(pos_vect[t] - pos_vect[r])**2-2) / ((pos_vect[t]-pos_vect[r])**2 - 1)**4\n",
    "    return 2* (A+B*C*D)\n",
    "\n",
    "def hess_F(pos_vect,cor_mtx):\n",
    "    n = len(pos_vect)\n",
    "    hess = np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            hess[i,j] = hess_F_xr_xt(pos_vect,cor_mtx,r=j,t=i)\n",
    "    return hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb3ccc30-0e10-4500-bd5f-7c8662b82253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main functions\n",
    "\n",
    "# The continuous optimize function:\n",
    "def continuous_optimize(data,x0_dict = 'random',show=False, method = 'total_adj', node_labels=False,\n",
    "                        almost_adj_weight = 0.2, fast_binary=False, save=[False,False,False],\n",
    "                        data_colors=False, max_step=0, pres_style='simple'):\n",
    "    \n",
    "    if type(data) == np.ndarray:\n",
    "        cor_mtx=data\n",
    "        nodes=list(range(cor_mtx.shape[0]))\n",
    "        if type(node_labels) not in [str,bool]: #ALTERED HERE\n",
    "            nodes=node_labels\n",
    "        num_nodes=len(nodes)\n",
    "    else:\n",
    "        if type(data) == list:\n",
    "            G=seqs_to_cor(data, method = method, almost_adj_weight = almost_adj_weight)\n",
    "        else:\n",
    "            G=data\n",
    "\n",
    "        # if graph is unweighted, assign unit weights\n",
    "        if nx.is_weighted(G) == False:\n",
    "            for edge in G.edges():\n",
    "                G[edge[0]][edge[1]]['weight']=1.0\n",
    "\n",
    "        nodes = list(G.nodes())\n",
    "        nodes.sort()\n",
    "        num_nodes = len(nodes)\n",
    "        cor_mtx = nx.to_numpy_matrix(G, nodelist=nodes)\n",
    "    \n",
    "    if method == 'total_adj' or np.max(cor_mtx) > 1:\n",
    "        cor_mtx = cor_mtx / np.max(cor_mtx) # * 0.98  #max correlation is 98%, linearly scale otherwise\n",
    "    \n",
    "    initial_guess = np.zeros(num_nodes)\n",
    "    if x0_dict == 'random':\n",
    "        for i in range(num_nodes):\n",
    "            initial_guess[i] = np.random.uniform(0,1)\n",
    "    else:\n",
    "        for i in range(num_nodes):\n",
    "            initial_guess[i] = x0_dict[nodes[i]]\n",
    "    \n",
    "    if fast_binary:\n",
    "        options = {'initial_trust_radius':0.1,'max_trust_radius':0.2,'maxiter':10**4}\n",
    "    else:\n",
    "        options = {'initial_trust_radius':0.02,'max_trust_radius':0.021,'maxiter':10**5}\n",
    "    if max_step != 0:\n",
    "        options = {'initial_trust_radius':.99*max_step,'max_trust_radius':max_step,'maxiter':10**5}\n",
    "    \n",
    "    opt_results = scipy.optimize.minimize(objective_function,args = (cor_mtx),x0=initial_guess,\n",
    "                            method = 'trust-ncg',jac = partial_F,\n",
    "                            hess = hess_F,\n",
    "                            options = options)\n",
    "    opt_pos = opt_results.x\n",
    "    opt_pos = opt_pos - np.min(opt_pos) + 0.5\n",
    "    score = opt_results.fun\n",
    "    \n",
    "    if show == False:\n",
    "        return opt_pos , nodes , score\n",
    "    \n",
    "    graph(cor_mtx, weighted = True, normalize = True, layout='circular', save=save[0],node_labels=nodes)\n",
    "    print('Initial Guess:')\n",
    "    generate_seqs(initial_guess,nodes,num_seqs = 0, show = True, save=save[1],data_colors=data_colors,pres_style=pres_style)\n",
    "    print()\n",
    "    print('Final Optimization:')\n",
    "    generate_seqs(opt_pos,nodes,num_seqs = 0, show = True, save=save[2],data_colors=data_colors,pres_style=pres_style)\n",
    "    print('Score:',score)\n",
    "    print('Settled in local min:',opt_results.success)\n",
    "    print('Num iterations:',opt_results.nit)\n",
    "    \n",
    "    return opt_pos , nodes , score\n",
    "\n",
    "#method for optimization using the Fiedler eigenvalue\n",
    "def eig_opt(matrix, show=False, labels=False, data_colors=True, normalize=True):\n",
    "    if normalize:\n",
    "        if matrix.shape[0]>0:\n",
    "            mtx=matrix / np.max(matrix)\n",
    "        else:\n",
    "            mtx=matrix\n",
    "    else:\n",
    "        mtx=matrix\n",
    "    L = np.diag(np.sum(mtx,axis=0)) - mtx\n",
    "    w,v = eigh(L)\n",
    "    sortWInds = np.argsort(w)\n",
    "    fVec = v[:,sortWInds[1]]\n",
    "    order=np.argsort(fVec)\n",
    "    sorted_mtx=np.zeros(mtx.shape)\n",
    "    for i in range(len(order)):\n",
    "        for j in range(len(order)):\n",
    "            sorted_mtx[i,j]=mtx[order[i],order[j]]\n",
    "    \n",
    "    if labels==False:\n",
    "        sorted_labels=order\n",
    "    else:\n",
    "        sorted_labels=[]\n",
    "        for i in order:\n",
    "            sorted_labels.append(labels[i])\n",
    "    \n",
    "    if show:\n",
    "        generate_seqs(np.sort(fVec)/(np.max(fVec)-np.min(fVec))*len(sorted_labels)/2.5,sorted_labels,num_seqs=0,show=True,data_colors=data_colors)\n",
    "        print('Eig Opt Score:',w[sortWInds[1]])\n",
    "    return w[sortWInds[1]] , fVec , sorted_labels, sorted_mtx\n",
    "\n",
    "#seeding the continuous optimization minimizer with the result of eigen optimization\n",
    "def eig_to_cont_opt(mtx, show=False, labels=False, data_colors=False, rtn='all',\n",
    "                    normalize=True, show_process=False, normalize_cscore=True,\n",
    "                    max_step=0,save=False, seed_with='order', start_with=False, pres_style='simple'):\n",
    "    \n",
    "    eig_score, start_locs, nodes, sorted_mtx = eig_opt(mtx,labels=labels)\n",
    "    start_locs=np.sort(start_locs)\n",
    "    \n",
    "    if seed_with=='pos':\n",
    "        #Scrunch together so most node pairs provide feedback\n",
    "        start_locs = start_locs-np.min(start_locs)\n",
    "        start_locs = start_locs / np.max(start_locs) / 1.5\n",
    "        seed_dict={}\n",
    "        for i in range(len(nodes)):\n",
    "            seed_dict[nodes[i]]=start_locs[i]\n",
    "    elif seed_with=='order':\n",
    "        equi_spaced=np.arange(len(nodes))/10\n",
    "        seed_dict={}\n",
    "        for i in range(len(nodes)):\n",
    "            if type(nodes)==np.ndarray:\n",
    "                seed_dict[nodes[i]]=equi_spaced[np.where(nodes == nodes[i])[0][0]]\n",
    "            else:\n",
    "                seed_dict[nodes[i]]=equi_spaced[nodes.index(nodes[i])]\n",
    "    \n",
    "    \n",
    "\n",
    "    c_opt_pos , nodes , c_score = continuous_optimize(sorted_mtx,x0_dict = seed_dict,\n",
    "                                                      show=show_process,node_labels=nodes,\n",
    "                                                      data_colors=data_colors,max_step=max_step)\n",
    "    \n",
    "    \n",
    "    # so that matrices of differing sizes can be compared\n",
    "    if normalize_cscore:\n",
    "        n=len(nodes)\n",
    "        comps = n*(n-1)/2\n",
    "        c_score = c_score / comps\n",
    "        \n",
    "    #standardize so that the endpt with smaller value is first, and arrays are in order\n",
    "    c_sorted_nodes=np.array(nodes)[np.argsort(c_opt_pos)]\n",
    "    c_opt_pos=np.sort(c_opt_pos)\n",
    "    if start_with==False and c_sorted_nodes[0] > c_sorted_nodes[-1]:\n",
    "        c_sorted_nodes=np.flip(c_sorted_nodes)\n",
    "        c_opt_pos=-np.flip(c_opt_pos)+c_opt_pos[-1]+c_opt_pos[0]\n",
    "        nodes=nodes[::-1]\n",
    "    elif start_with!=False and list(c_sorted_nodes).index(start_with)>.5*len(c_sorted_nodes):\n",
    "        c_sorted_nodes=np.flip(c_sorted_nodes)\n",
    "        c_opt_pos=-np.flip(c_opt_pos)+c_opt_pos[-1]+c_opt_pos[0]\n",
    "        nodes=nodes[::-1]\n",
    "    \n",
    "    if show:\n",
    "        print('Eigen Order:',nodes)\n",
    "        print('Final Optimization:')\n",
    "        generate_seqs(c_opt_pos,c_sorted_nodes,num_seqs = 0, show = True, save=save, data_colors=data_colors,pres_style=pres_style)\n",
    "        print('Eigenvalue Optimization Score:', eig_score)\n",
    "        #print('Eigen Order:',nodes)\n",
    "        print('Continuous Optimization Score:',c_score)\n",
    "        print(list(c_sorted_nodes))\n",
    "    else:\n",
    "        if rtn == 'all':\n",
    "            return eig_score, nodes , c_score , c_opt_pos , c_sorted_nodes\n",
    "        elif rtn == 'scores':\n",
    "            return eig_score , c_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1380eb31-8b80-413b-9d0d-2b5d8a5339a4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Simulating place cell spike train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83231af8-23e7-4185-b474-26682a1ec900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def two_dim_bump(pf_center,pos,m,n,w):\n",
    "    d=np.linalg.norm(pf_center-pos)\n",
    "    if d>=w:\n",
    "        return n\n",
    "    else:\n",
    "        return (m-n)*np.exp(-d**2 / (w**2 - d**2))+n\n",
    "    \n",
    "def pt_inside_poly(point,boundry_poly_vertices):\n",
    "    sides=[]\n",
    "    for i in range(boundry_poly_vertices.shape[0]-1):\n",
    "        sides.append(np.array([boundry_poly_vertices[i,:],boundry_poly_vertices[i+1,:]]))\n",
    "    sides.append(np.array([boundry_poly_vertices[-1,:],boundry_poly_vertices[0,:]]))\n",
    "    avoid_angles=[]\n",
    "    for side in sides:\n",
    "        if side[1,0]==side[0,0]:\n",
    "            avoid_angles.append(1.57)\n",
    "        else:\n",
    "            avoid_angles.append(np.arctan((side[1,1]-side[0,1])/(side[1,0]-side[0,0])))\n",
    "    for i in range(boundry_poly_vertices.shape[0]):\n",
    "        if point[0]==boundry_poly_vertices[i,0]:\n",
    "            avoid_angles.append(1.57)\n",
    "        else:\n",
    "            avoid_angles.append(np.arctan((boundry_poly_vertices[i,1]-point[1])/(boundry_poly_vertices[i,0]-point[0])))\n",
    "    avoid_angles=np.sort(np.array(avoid_angles))\n",
    "    angle=avoid_angles[np.argmax(np.diff(avoid_angles))]+avoid_angles[np.argmax(np.diff(avoid_angles))+1]\n",
    "    angle=angle/2\n",
    "    ray=np.array([point,point+2*np.max(boundry_poly_vertices)*np.array([np.cos(angle),np.sin(angle)])])\n",
    "    intersections=0\n",
    "    for side in sides:\n",
    "        #plt.plot(ray[:,0],ray[:,1],color='k')\n",
    "        #plt.plot(side[:,0],side[:,1],color='b')\n",
    "        A=np.array([[ray[0,1]-ray[1,1] , ray[1,0]-ray[0,0]], [side[0,1]-side[1,1] , side[1,0]-side[0,0]] ])\n",
    "        b=np.array([(ray[1,0] - ray[0,0])*ray[0,1] - (ray[1,1]-ray[0,1])*ray[0,0] , (side[1,0] - side[0,0])*side[0,1] - (side[1,1]-side[0,1])*side[0,0]])\n",
    "        x=np.linalg.solve(A,b)\n",
    "        #c='k'\n",
    "        if np.min(ray[:,0])<x[0]<np.max(ray[:,0]) and np.min(ray[:,1])<x[1]<np.max(ray[:,1]):\n",
    "            #print('ray good')\n",
    "            #print(side,x)\n",
    "            if np.min(side[:,0])-10**-6<x[0]<np.max(side[:,0])+10**-6 and np.min(side[:,1])-10**-6<x[1]<np.max(side[:,1])+10**-6:\n",
    "                #print('and side good')\n",
    "                intersections+=1\n",
    "                #c='r'\n",
    "        #plt.scatter(x[0],x[1],color=c)\n",
    "        #plt.show()\n",
    "    if intersections%2 == 0:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def crosses_boundry(old_pt,new_pt,boundry_poly_vertices):\n",
    "    sides=[]\n",
    "    for i in range(boundry_poly_vertices.shape[0]-1):\n",
    "        sides.append(np.array([boundry_poly_vertices[i,:],boundry_poly_vertices[i+1,:]]))\n",
    "    sides.append(np.array([boundry_poly_vertices[-1,:],boundry_poly_vertices[0,:]]))\n",
    "    ray=np.array([old_pt,new_pt])\n",
    "    for side in sides:\n",
    "        A=np.array([[ray[0,1]-ray[1,1] , ray[1,0]-ray[0,0]], [side[0,1]-side[1,1] , side[1,0]-side[0,0]] ])\n",
    "        b=np.array([(ray[1,0] - ray[0,0])*ray[0,1] - (ray[1,1]-ray[0,1])*ray[0,0] , (side[1,0] - side[0,0])*side[0,1] - (side[1,1]-side[0,1])*side[0,0]])\n",
    "        x=np.linalg.solve(A,b)\n",
    "        if np.min(ray[:,0])-10**-6<x[0]<np.max(ray[:,0])+10**-6 and np.min(ray[:,1])-10**-6<x[1]<np.max(ray[:,1])+10**-6:\n",
    "            if np.min(side[:,0])-10**-6<x[0]<np.max(side[:,0])+10**-6 and np.min(side[:,1])-10**-6<x[1]<np.max(side[:,1])+10**-6:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b730d3f-8318-45c6-9733-e20ba61bbf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function\n",
    "\n",
    "def generate_2d_environment_spikes(pf_centers,pf_max_rates=30,pf_rads=1,labels=False,\n",
    "                                   boundry_poly_vertices=False, hole_vertices=[],\n",
    "                                   start_pos='random',travel_length='random', num_runs=10, momentum=1,\n",
    "                                   show=False, noise=1, run_speed=.8, figsize='auto',save=False):\n",
    "    if labels==False:\n",
    "        labels=np.arange(max(pf_centers.shape))\n",
    "    if type(pf_rads) in [int,float]:\n",
    "        pf_rads=pf_rads*np.ones(len(labels))\n",
    "    x_min , x_max = np.min(pf_centers[:,0])-np.max(pf_rads) , np.max(pf_centers[:,0])+np.max(pf_rads)\n",
    "    y_min , y_max = np.min(pf_centers[:,1])-np.max(pf_rads) , np.max(pf_centers[:,1])+np.max(pf_rads)\n",
    "    if type(boundry_poly_vertices)==bool:\n",
    "        boundry_poly_vertices=np.array([[x_min,y_min],[x_min,y_max],[x_max,y_max],[x_max,y_min]])\n",
    "    if type(noise) in [int,float]:\n",
    "        neuron_noises=noise*np.ones(len(labels))\n",
    "    else:\n",
    "        neuron_noises=noise\n",
    "    if type(pf_max_rates) in [int,float]:\n",
    "        pf_max_rates=pf_max_rates*np.ones(len(labels))\n",
    "    if type(pf_rads) in [int,float]:\n",
    "        pf_rads=pf_rads*np.ones(len(labels))\n",
    "        \n",
    "    paths=[]\n",
    "    max_run_time=0\n",
    "    for c in range(num_runs):\n",
    "        if type(travel_length)==str:\n",
    "            run_time=np.random.randint(15,25)\n",
    "        else:\n",
    "            if type(travel_length) in [float,int]:\n",
    "                run_time=round(travel_length)\n",
    "            else:\n",
    "                run_time=round(travel_length[c])\n",
    "        if run_time>max_run_time:\n",
    "            max_run_time=run_time\n",
    "        if type(start_pos)==str:\n",
    "            found=False\n",
    "            while found==False:\n",
    "                pos=np.array([np.random.uniform(np.min(boundry_poly_vertices[:,0]),np.max(boundry_poly_vertices[:,0]))\n",
    "                              ,np.random.uniform(np.min(boundry_poly_vertices[:,1]),np.max(boundry_poly_vertices[:,1]))])\n",
    "                outside_holes=True\n",
    "                for hole in hole_vertices:\n",
    "                    if outside_holes:\n",
    "                        if pt_inside_poly(pos,hole):\n",
    "                            outside_holes=False\n",
    "                if pt_inside_poly(pos,boundry_poly_vertices) and outside_holes:\n",
    "                    found=True\n",
    "                    cur_pos=pos\n",
    "        else:\n",
    "            cur_pos=start_pos\n",
    "        \n",
    "        #start with course 1 second time scale to construct position as function of time\n",
    "        coarse_positions=np.zeros((run_time,2))\n",
    "        coarse_positions[0,:]=cur_pos\n",
    "        cur_dir= np.random.normal(get_dir(cur_pos,.5*np.array([x_min+x_max,y_min+y_max])),2)\n",
    "        for t in range(1,run_time):\n",
    "            found_new = False\n",
    "            tries = 0\n",
    "            while found_new == False:\n",
    "                step_length=np.abs(np.random.normal(run_speed,.15))\n",
    "                new_dir = cur_dir + np.random.normal(0,1.4/(momentum+10**-5))\n",
    "                new_pos = cur_pos + step_length * np.array([np.cos(new_dir),np.sin(new_dir)])\n",
    "                outside_holes=True\n",
    "                for hole in hole_vertices:\n",
    "                    if outside_holes:\n",
    "                        if crosses_boundry(coarse_positions[t-1,:],new_pos,hole)==True:\n",
    "                            outside_holes=False\n",
    "                if crosses_boundry(coarse_positions[t-1,:],new_pos,boundry_poly_vertices)==False and outside_holes:\n",
    "                    found_new = True\n",
    "                    coarse_positions[t,:]=new_pos\n",
    "                    cur_pos , cur_dir = new_pos , new_dir\n",
    "                elif tries > 100:\n",
    "                    found_rot = False\n",
    "                    rot = 0\n",
    "                    while found_rot == False:\n",
    "                        new_dir = cur_dir + (-1)**rot * .05 * rot\n",
    "                        new_pos = cur_pos + step_length * np.array([np.cos(new_dir),np.sin(new_dir)])\n",
    "                        outside_holes=True\n",
    "                        for hole in hole_vertices:\n",
    "                            if outside_holes:\n",
    "                                if crosses_boundry(coarse_positions[t-1,:],new_pos,hole)==True:\n",
    "                                    outside_holes=False\n",
    "                        if crosses_boundry(coarse_positions[t-1,:],new_pos,boundry_poly_vertices)==False and outside_holes:\n",
    "                            found_new = True\n",
    "                            found_rot = True\n",
    "                            coarse_positions[t,:]=new_pos\n",
    "                            cur_pos , cur_dir = new_pos , new_dir\n",
    "                        rot+=1\n",
    "                tries+=1\n",
    "            \n",
    "        s=10\n",
    "        #now make positions for every 1/s seconds and add some granular noise\n",
    "        fine_positions=np.zeros((coarse_positions.shape[0]*s,2))\n",
    "        for i in range(coarse_positions.shape[0]):\n",
    "            fine_positions[s*i,:]=coarse_positions[i,:]\n",
    "        #linear interpolation\n",
    "        for i in range(coarse_positions.shape[0]-1):\n",
    "            fine_positions[s*i:s*i+s,0]=np.linspace(fine_positions[s*i,0],fine_positions[s*i+s,0],s)\n",
    "            fine_positions[s*i:s*i+s,1]=np.linspace(fine_positions[s*i,1],fine_positions[s*i+s,1],s)\n",
    "        fine_positions=fine_positions[:-(s-1),:]\n",
    "        #adding noise\n",
    "        fine_positions+=np.random.normal(0,.01,fine_positions.shape)\n",
    "        #smoothing\n",
    "        fine_positions[:,0]=moving_mean(fine_positions[:,0],2)\n",
    "        fine_positions[:,1]=moving_mean(fine_positions[:,1],2)\n",
    "        paths.append(fine_positions)\n",
    "        \n",
    "    #now for some spike times given the positions\n",
    "    spike_times=[ [] for _ in range(len(labels)) ]\n",
    "    spike_pos=[ [] for _ in range(len(labels)) ]\n",
    "    start_time_gap=100\n",
    "    if max_run_time>start_time_gap:\n",
    "        start_time_gap=round(max_run_time)+20\n",
    "    start_time=-start_time_gap\n",
    "    for i in range(len(labels)):\n",
    "        m,n,w,c=pf_max_rates[i],neuron_noises[i],pf_rads[i],pf_centers[i,:]\n",
    "        for p in range(len(paths)):\n",
    "            start_time=p*start_time_gap\n",
    "            for t in range(paths[p].shape[0]):\n",
    "                timestamp=start_time+(t+1)/s\n",
    "                expected_fires=.5/s * two_dim_bump(c,paths[p][t,:],m,n,w) #????\n",
    "                if expected_fires<1:\n",
    "                    if expected_fires>np.random.uniform(0,1):\n",
    "                        expected_fires=1\n",
    "                    else:\n",
    "                        expected_fires=0\n",
    "                expected_fires=round(abs(expected_fires*np.random.normal(1,neuron_noises[i])))\n",
    "                for k in range(expected_fires):\n",
    "                    spike_times[i].append(np.random.uniform(timestamp-1/s,timestamp+1/s))\n",
    "                    spike_pos[i].append(paths[p][t,:]+np.random.normal(0,.02,2))\n",
    "        spike_times[i].sort()\n",
    "        spike_times[i]=np.array(spike_times[i])\n",
    "        spike_pos[i]=np.array(spike_pos[i])\n",
    "    \n",
    "        \n",
    "    if show:\n",
    "        if type(figsize) != str:\n",
    "            plt.figure(figsize=figsize, dpi=150)\n",
    "        else:\n",
    "            plt.figure(figsize=(10,10*np.max(boundry_poly_vertices[:,1])/np.max(boundry_poly_vertices[:,0])))\n",
    "        plt.plot(boundry_poly_vertices[:,0],boundry_poly_vertices[:,1],color='k',alpha=0.5)\n",
    "        plt.plot(boundry_poly_vertices[[-1,0],0],boundry_poly_vertices[[-1,0],1],color='k',alpha=0.5)\n",
    "        for hole in hole_vertices:\n",
    "            plt.plot(hole[:,0],hole[:,1],color='k',alpha=0.5)\n",
    "            plt.plot(hole[[-1,0],0],hole[[-1,0],1],color='k',alpha=0.5)\n",
    "        \n",
    "        for i in range(len(paths)):\n",
    "            plt.plot(paths[i][:,0],paths[i][:,1],color='k',alpha=0.5)\n",
    "        if num_runs>0:\n",
    "            for i in range(len(labels)):\n",
    "                plt.scatter(spike_pos[i][:,0],spike_pos[i][:,1],color=color_list[labels[i]%20],label=labels[i],alpha=0.5)\n",
    "        for i in range(len(paths)):\n",
    "            plt.plot(paths[i][0,0],paths[i][0,1],marker = TextPath((0,0), alphabet[i]), markersize=20*np.sqrt(len(str(i))), color='k')\n",
    "        plt.axis('equal')\n",
    "        for i in range(len(labels)):\n",
    "            plt.plot(pf_centers[i,0],pf_centers[i,1],marker=TextPath((0,0), str(labels[i])),linewidth=1, markeredgewidth=.4,markeredgecolor='k', color=color_list[labels[i]%20], markersize=25*np.sqrt(len(str(labels[i]))))\n",
    "            for j in range(1,20):\n",
    "                circle = plt.Circle((pf_centers[i,0],pf_centers[i,1]), 1/20*j, color=color_list[labels[i]%20], fill=True, alpha=(.22/j+.01)*pf_max_rates[i]/np.mean(pf_max_rates))\n",
    "                ax=plt.gca()\n",
    "                ax.add_patch(circle)\n",
    "            circle_border = plt.Circle((pf_centers[i,0],pf_centers[i,1]), 1, color=color_list[labels[i]%20], fill=False, alpha=0.15)\n",
    "            ax=plt.gca()\n",
    "        plt.axis('off')\n",
    "        if save != False:\n",
    "            plt.savefig(save,dpi=150)\n",
    "        plt.show()\n",
    "        \n",
    "    return spike_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3ddf908-cf61-4188-a36d-4c04060167eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environments used in dissertation\n",
    "# Defining the linear track, Y, X, circular track, \n",
    "# two dimensional box, holey octagon environments, as well as just noise.\n",
    "\n",
    "def set_up_environments(n=21):\n",
    "\n",
    "    # linear track\n",
    "    lin_boundry=np.array([[0,0],[.5,0],[.5,1.1*n],[0,1.1*n]])\n",
    "    lin_pf=np.ones((n,2))*.25\n",
    "    lin_pf[:,1]=np.linspace(.4,1.1*n-.4,n)\n",
    "\n",
    "    # box environment\n",
    "    # normal square lattice\n",
    "    if n != 21:\n",
    "        h=round(np.floor(n**.5))\n",
    "        w=round(n/h)\n",
    "        box_boundry=np.array([[0,0],[1.1*w,0],[1.1*w,1.1*h],[0,1.1*h]])\n",
    "        xs = np.linspace(0.4, 1.1*w-.4, w)\n",
    "        ys = np.linspace(0.4, 1.1*h-.4, h)\n",
    "        box_pf=[]\n",
    "        for i in range(w):\n",
    "            for j in range(h):\n",
    "                box_pf.append([xs[i],ys[j]])\n",
    "        box_pf=np.array(box_pf)\n",
    "    #special case for dissertation, triangular latice\n",
    "    else:\n",
    "        h=3\n",
    "        s3=np.sqrt(3)\n",
    "        w=5*s3/2\n",
    "        x1s = np.array([0,s3,2*s3])\n",
    "        y1s = np.array([0,1,2,3])\n",
    "        box_pf=[]\n",
    "        for i in range(3):\n",
    "            for j in range(4):\n",
    "                box_pf.append([x1s[i],y1s[j]])\n",
    "                if j<3:\n",
    "                    box_pf.append([x1s[i]+s3/2,y1s[j]+0.5])\n",
    "        box_pf=np.array(box_pf)\n",
    "        box_boundry=np.array([[-.3,-.3],[2.5*s3+.3,-.3],[2.5*s3+.3,3.3],[-.3,3.3]])\n",
    "        scale=1.2\n",
    "        box_pf*=scale\n",
    "        box_boundry*=scale\n",
    "        \n",
    "\n",
    "    # long loop\n",
    "    k=10\n",
    "    r=(n*1.1)/2/np.pi\n",
    "    inner=r*np.array([np.cos(np.linspace(0,2*np.pi,k+1)),np.sin(np.linspace(0,2*np.pi,k+1))]).T\n",
    "    outer=(r+.5)/r*inner\n",
    "    loop_pf=r*np.array([np.cos(np.linspace(0,(n-1)/n*2*np.pi,n)),np.sin(np.linspace(0,(n-1)/n*2*np.pi,n))]).T\n",
    "\n",
    "    # Y shape\n",
    "    leg=round(n/3)\n",
    "    s=1.1*leg\n",
    "    q3=3**.5\n",
    "    y_boundry=np.array([[-.25,0],[-.25,s],[-.25-s*q3/2,s+s/2],[-.25-s*q3/2+.25,s+s/2+q3/4],[0,s+q3/4],\n",
    "                                    [.25+s*q3/2-.25,s+s/2+q3/4],[.25+s*q3/2,s+s/2],[.25,s],[.25,0]])\n",
    "    y_pf=np.zeros((3*leg,2))\n",
    "    y_pf[0:leg,1]=np.linspace(.3,s-.5,leg)\n",
    "    y_pf[leg:2*leg,0]=np.linspace(-s*q3/2+.2,-.4,leg)\n",
    "    y_pf[leg:2*leg,1]=np.linspace(s+s/2,s+.4,leg)\n",
    "    y_pf[2*leg:,0]=np.linspace(s*q3/2-.2,.4,leg)\n",
    "    y_pf[2*leg:,1]=np.linspace(s+s/2,s+.4,leg)\n",
    "\n",
    "    #Holey Octagon\n",
    "    ho_boundry=np.array([[0,0],[-1,1],[-1,2],[0,3],[3,3],[4,2],[4,1],[3,0]])*2\n",
    "    ho_hole=[np.array([[0,1],[1.5,1],[.75,2.5]])*1.1+.3,np.array([[4,1.5],[5.6,1.5],[5.6,3],[4,3]]),\n",
    "                   np.array([[1.7,5],[3.6,5.2],[3,4]])]\n",
    "    ho_boundry=ho_boundry*.8\n",
    "    for i in range(len(ho_hole)):\n",
    "        ho_hole[i]=ho_hole[i]*.8\n",
    "\n",
    "    ho_pf=[]\n",
    "    for i in np.arange(-1.2,7,1.2):\n",
    "        for j in np.arange(0.5,5,1.2):\n",
    "            if pt_inside_poly(np.array([i,j]),ho_boundry):\n",
    "                outside_holes=True\n",
    "                for hole in ho_hole:\n",
    "                    if outside_holes:\n",
    "                        if pt_inside_poly(np.array([i,j]),hole):\n",
    "                            outside_holes=False\n",
    "                if outside_holes:\n",
    "                    ho_pf.append([i,j])\n",
    "    ho_pf=np.array(ho_pf)\n",
    "\n",
    "    #Random (place fields outside environment)\n",
    "    rand_boundry=np.array([[0,0],[1000,0],[1000,1000],[0,1000]])\n",
    "    rand_pf=np.ones((n,2))*-10\n",
    "    \n",
    "    # cross shape\n",
    "    m=np.max([round((n+1)/4),2])\n",
    "    cross_boundry=np.array([[0,0],[0,m],[-m,m],[-m,m+.5],[0,m+.5],[0,2*m+.5],\n",
    "                                [.5,2*m+.5],[.5,m+.5],[m+.5,m+.5],[m+.5,m],[.5,m],\n",
    "                                [.5,0]])\n",
    "    cross_boundry[:,0]+=-.25*np.ones(cross_boundry.shape[0])\n",
    "    cross_pf=np.zeros((4*m-3,2))\n",
    "    cross_pf[0:2*m-1,1]=np.linspace(.3,2*m+.2,2*m-1)\n",
    "    cross_pf[2*m-1:,1]=m+.25\n",
    "    cross_pf[2*m-1:3*m-2,0]=np.linspace(-m+.3,-1,m-1)\n",
    "    cross_pf[3*m-2:,0]=np.linspace(m-.3,1,m-1)\n",
    "    \n",
    "    str_list=['lin','box','loop','y','ho','rand','cross']\n",
    "    \n",
    "    bdry={}\n",
    "    bdry['lin'],bdry['box'],bdry['loop'],bdry['rand']=lin_boundry,box_boundry,outer,rand_boundry\n",
    "    bdry['y'],bdry['ho'],bdry['cross']=y_boundry,ho_boundry,cross_boundry\n",
    "    \n",
    "    holes={}\n",
    "    holes['lin'],holes['box'],holes['loop'],holes['rand']=[],[],[inner],[]\n",
    "    holes['y'],holes['ho'],holes['cross']=[],ho_hole,[]\n",
    "    \n",
    "    pfs={}\n",
    "    pfs['lin'],pfs['box'],pfs['loop'],pfs['rand']=lin_pf,box_pf,loop_pf,rand_pf\n",
    "    pfs['y'],pfs['ho'],pfs['cross']=y_pf,ho_pf,cross_pf\n",
    "    \n",
    "    return str_list,bdry,holes,pfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf372c7-d35b-4281-b8e5-77b467e048db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method and parameter choices for generating the data for the dissertation\n",
    "# will take time to reexecute\n",
    "\n",
    "#session params\n",
    "sessions=1000\n",
    "runs_per_ses=20\n",
    "\n",
    "#navigation params\n",
    "momentum=2\n",
    "run_speed=.8\n",
    "noise=1\n",
    "\n",
    "all_env=set_up_environments(n=21)\n",
    "\n",
    "for s in ['lin','box','loop','y','ho','rand','cross']:\n",
    "    for ses in range(sessions):\n",
    "        spike_times=generate_2d_environment_spikes(all_env[3][s],boundry_poly_vertices=all_env[1][s],\n",
    "                                 hole_vertices=all_env[2][s], momentum=momentum, run_speed=run_speed,\n",
    "                                 num_runs=runs_per_ses,show=False, noise=noise)\n",
    "        co_mtx=discrete_correlation(spike_times)\n",
    "        df=pd.DataFrame(co_mtx)\n",
    "        \n",
    "        #data then saved here for later analysis\n",
    "        #df.to_csv('coactivity_csvs/'+s+'/'+s+'_'+str(ses)+'LONG.csv', index=False, header=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
